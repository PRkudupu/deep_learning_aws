{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine and Deep Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook comprises a quick, non-exhaustive introduction to fundamental concepts in both machine and deep learning. We aim to familiarize you with these concepts in the hopes that you will build upon them to further your own understanding and capabilities in machine and deep learning.\n",
    "\n",
    "### What's Covered\n",
    "Key concepts covered in this tutorial include:\n",
    "* Regression\n",
    "* Classification\n",
    "* Loss Functions\n",
    "* Validation & Overfitting \n",
    "* Hyperparameter tuning\n",
    "* Gradient Descent & Optimization\n",
    "* Neural Networks\n",
    "\n",
    "### What's Not\n",
    "We will use a variety of different python libraries in this tutorial, including specific frameworks for data processing and machine and deep learning. The syntax, functionality, and APIs behind these frameworks is not our focus for this introduction. However, we encourage you to learn more about them, as they are essential components of the ML practitioner's toolkit.\n",
    "\n",
    "These libraries include:\n",
    "* [numpy](https://docs.scipy.org/doc/numpy-1.15.1/reference/index.html)\n",
    "* [pandas](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [scikit-learn](https://scikit-learn.org/stable/)\n",
    "* [tensorflow](https://www.tensorflow.org/)\n",
    "* [keras](https://keras.io/)\n",
    "\n",
    "### What's Expected\n",
    "\n",
    "This tutorial will assume you have some experience with numpy and data structures.\n",
    "\n",
    "## Contents\n",
    "\n",
    "This tutorial consists of three parts:\n",
    "\n",
    "* Machine Learning\n",
    " * Part 1: Linear Regression\n",
    " * Part 2: Decision Tree Classification\n",
    "* Deep Learning \n",
    " * Multi-Layer Perceptron (MLP)\n",
    "\n",
    "## Jupyter Notebook\n",
    "\n",
    "This tutorial is executed in a specific IDE call a **Jupyter Notebook**. If you know what a Jupyter Notebook is, please **skip to the next section**.\n",
    "\n",
    "A Jupyter Notebook is \"[an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text](https://jupyter.org/). It's an IDE that is commonly used by Data Scientists that simplifies exploratory data analysis (EDA) and experimentation. \n",
    "\n",
    "The notebook application allows you to execute **code cells** interactively. As an example, execute the code cell below by selecting the cell and clicking the **Run** button above (or using the appropriate hot key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook executes the code cell, and prints outputs beneath the code cell.\n",
    "\n",
    "The notebook sits on top of a python kernel that maintains state throughout the lifetime of the notebook. When we execute the code block below, we assign 5 and 6 to variables <tt>a</tt> and <tt>b</tt> respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 6\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we run the next cell block repeatedly, we get incrementally higher values of <tt>a</tt> because the kernel is keeping track of the value of <tt>a</tt> as if you were running this in any other environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important when working through the tutorial notebook to remember that you need to execute blocks in order as if they were in a script.\n",
    "\n",
    "The numbers in brackets next to each code cell indicate the relative order of execution; while a block is executing, you'll see an asterisk **\\*** in the brackets.\n",
    "\n",
    "As an example, execute the next cell. This will kick off an infinite loop, but you can interrupt the kernel at anytime by pressing the **stop** button above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install python-graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes, load_iris, load_breast_cancer\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "# %matplotlib inline\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "Before we start looking at machine and deep learning, it's important to understand the relationship between each and AI as a whole. Artifical intelligence, machine learning, and deep learning overlap, and subset each other from right to left. Let's broadly define each here:\n",
    "* **Artificial Intelligence**: Techniques and methods used to enable computer systems to perform complex tasks that are perceivable as *intelligent*.\n",
    "* **Machine Learning**: Algorithms and statistical models that accomplish the above via modeling and inference techniques, as opposed to *rule-based* systems\n",
    "* **Deep Learning**: A class of machine learning algorithms called *neural networks* that are state-of-the-art at many tasks such as computer-vision and natural language processing (NLP) \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/36491325/51944421-de39a400-23d0-11e9-8fde-28a2f75b2a7f.png\" width=\"400\" height=\"400\" />\n",
    "\n",
    "Simply put, machine learning models perform intelligent tasks and provide intelligent insights by *learning* them from **data**, instead of relying on hand-crafted rules and logic. Deep learning concerns a particular class of models called neural networks. Managing and tuning this learning process is a core part of what it means to practice machine learning.\n",
    "\n",
    "There are many different machine learning models and different tasks. When those tasks are to make predictions, machine learning models typically learn how to do this by looking at examples; these are **observations** with corresponding **labels**. This method of learning is called **supervised learning**, and is our focus.\n",
    "\n",
    "In this section of this tutorial, we will focus on two different machine learning **algorithms/models** for two different machine learning **tasks**:\n",
    "* **linear models** for **regression**\n",
    "* **decision trees** for **classification**\n",
    "\n",
    "\n",
    "## Part 1: Linear Regression\n",
    "\n",
    "Using linear models for regression, or simply linear regression, has a long history in statistical analysis and is so well known that many don't realize it is in fact one of the simplest examples of machine learning. \n",
    "\n",
    "In **linear regression**, our model describes a linear relationship between a **label**, which we call a **target**, and a set of **observations**, which we call features. Recall the equation of a line: \n",
    "\n",
    "$y=mx + b$\n",
    "\n",
    "The **parameters** $m$ and $b$ describe the relationship between the **feature** $x$ and the **target** $y$. \n",
    "\n",
    "Let's create a function for this linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_model(params={\"m\":1.0, \"b\":0.0}):\n",
    "    return lambda x: params['m']*x + params['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the ML process works, we're going to work backwards from a working model.\n",
    "\n",
    "### Linear Model with Known Parameters\n",
    "\n",
    "Let's say we want to create a linear model to describe the relationship between the age of adolescent boys and their height. **Assume** we know that the average height of boys increases by 2 inches for each year between 11 and 18, and the average height at age 11 is 55 inches.\n",
    "\n",
    "If feature $x$ represents years after 11, and $y$ represents height in inches, then we can say that parameter $m$ is 0.5 and the parameter $b$ is 55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boy_height_params = {\"m\":2.0, \"b\":55.0}\n",
    "boy_height_linear_model = create_linear_model(boy_height_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our model to make a prediction on an input feature. This process of making predictions on new values is called **inference**.\n",
    "\n",
    "**Try predicting for different values of <tt>x</tt> between 0 and 7:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "y = boy_height_linear_model(x)\n",
    "print(f\"Linear model predicted a height of {y // 12} ft, {y % 12} inches for a {x+11}-year-old boy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this line over a range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.set_xlabel(\"Years over 11\")\n",
    "ax.set_ylabel(\"in\")\n",
    "ax.set_title(\"Average Height of Teenage Boy\")\n",
    "ax.plot(list(range(8)), [boy_height_linear_model(i) for i in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we have a working linear model that describes the relationship between teenage boys age and their average height. \n",
    "\n",
    "### Linear Model with Unknown Parameters\n",
    "\n",
    "We got the model in the previous section because we were able to figure out what the **parameters** $m$ and $b$ are, based on an **assumption** we made about the relationship between the feature $x$ and target $y$. In reality, we never know what the right parameters are, we have to **learn** them by looking at historical data or observations.\n",
    "\n",
    "Let's look at an example of what real data would look like by loading a sample diabetes dataset from the sci-kit learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_dataset = load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the description of the dataset features (called attributes) and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diabetes_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a linear model to describe the relationship between body mass index (our feature $x$) and diabetes progression (the target $y$). But this time, we have no idea what the parameters should be.\n",
    "\n",
    "Let's look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_dataset['data'][:,np.newaxis,2]\n",
    "y = diabetes_dataset['target'][:,np.newaxis]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"BMI\")\n",
    "ax.set_ylabel(\"Diabetes Progression\")\n",
    "ax.scatter(X, y)\n",
    "# ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking good parameters\n",
    "\n",
    "Given that $m$ is the slope of the linear model and $b$ is the y-intercept, can you guess the parameters that would best fit the average diabetes progession? \n",
    "\n",
    "**Try by changing the parameter dictionary below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change m and b values below to visualize your line guess\n",
    "guess_params = {\"m\": INSERT_M, \"b\": INSERT_B}\n",
    "\n",
    "\n",
    "guess_line_function = create_linear_model(guess_params)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "# ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "start = np.min(X)\n",
    "stop = np.max(X)\n",
    "preds = np.array([guess_line_function(x) for x in X])\n",
    "ax.set_title(f\"Linear model y = {guess_params['m']}*x + {guess_params['b']}\")\n",
    "ax.set_xlabel(\"BMI\")\n",
    "ax.set_ylabel(\"Diabetes Progression\")\n",
    "ax.plot(X, preds, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to qualify which parameters are good guesses, and which are bad. To do this, we have to use a **loss function**, a function that compares how far off predicted targets are from the real targets. Good parameters should give predictions that have a small loss, while bad parameters should give a large loss.\n",
    "\n",
    "For regression, we often use the **mean squared error**, or **mse** as a loss function. This is the average squared difference between predictions and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(preds, y):\n",
    "    return np.mean((preds - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a way of measuring how good our model is performing, let's try again to find the best parameters. See if you can find them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change m and b values below to visualize your guess and get a loss value\n",
    "guess_params = {\"m\":INSERT_M, \"b\":INSERT_B}\n",
    "\n",
    "\n",
    "guess_line_function = create_linear_model(guess_params)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "# ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "start = np.min(X)\n",
    "stop = np.max(X)\n",
    "preds = np.array([guess_line_function(x) for x in X])\n",
    "ax.set_title(f\"Linear model y = {guess_params['m']}*x + {guess_params['b']}, MSE = {np.round(mse(preds, y), decimals=1)}\")\n",
    "ax.set_xlabel(\"BMI\")\n",
    "ax.set_ylabel(\"Diabetes Progression\")\n",
    "ax.plot(X, preds, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning the best parameters\n",
    "\n",
    "Rather than guessing, machine learning let's us *learn* the correct parameters or algorithms for our models, given labeled data. Machine learning models find the parameters or algorithm that minimize the loss function between the data labels, and the predicted values. This process is called **training** or **fitting**.\n",
    "\n",
    "We can use the <tt>LinearRegression</tt> model in scikit-learn to fit a linear regression model to our data and find the best param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "print(f\"Best parameters: m = {lr_model.coef_[0,0]}, b = {lr_model.intercept_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\"m\":lr_model.coef_[0,0], \"b\":lr_model.intercept_[0]}\n",
    "\n",
    "best_line_function = create_linear_model(best_params)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "start = np.min(X)\n",
    "stop = np.max(X)\n",
    "preds = np.array([best_line_function(x) for x in X])\n",
    "ax.set_title(f\"Linear model y = {np.round(best_params['m'], 1)}*x + {np.round(best_params['b'], 1)},\\\n",
    "  MSE = {np.round(mse(preds, y), decimals=1)}\")\n",
    "ax.set_xlabel(\"BMI\")\n",
    "ax.set_ylabel(\"Diabetes Progression\")\n",
    "ax.plot(X, preds, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know these are the best parameters?\n",
    "\n",
    "If we test out a grid of parameters, and calculate their corresponding mse's, we can plot the loss surface as a function of parameters m and b:\n",
    "\n",
    "(You can grab the plot below and move it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "m_range = np.concatenate([np.linspace(best_params['m'] - 10, best_params['m'], num=10),\n",
    "                          np.linspace(best_params['m'] + 1, best_params['m'] + 11, num=10)])\n",
    "\n",
    "b_range = np.concatenate([np.linspace(best_params['b'] - 1, best_params['b'], num=10),\n",
    "                          np.linspace(best_params['b'], best_params['b'] + 1, num=10)])\n",
    "\n",
    "m_grid, b_grid = np.meshgrid(m_range, b_range)\n",
    "zs = []\n",
    "for m,b in zip(np.ravel(m_grid), np.ravel(b_grid)):\n",
    "    linear_model = create_linear_model({\"m\":m, \"b\":b})\n",
    "    tmp_preds = np.array([linear_model(x) for x in X])\n",
    "    zs.append(mse(tmp_preds, y))\n",
    "zs = np.array(zs)\n",
    "Z = zs.reshape(m_grid.shape)\n",
    "\n",
    "ax.plot_wireframe(m_grid, b_grid, Z)\n",
    "ax.scatter(best_params['m'], best_params['b'], mse(preds, y), s=100, marker=(5,1), c='r')\n",
    "ax.text(best_params['m'], best_params['b'], mse(preds, y), \"Best Parameters\", zorder=1)\n",
    "\n",
    "ax.set_xlabel('m Values')\n",
    "ax.set_ylabel('b Values')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression, the loss function is convex, and the best parameters are the global minima seen above. When scikit-learns's <tt>LinearRegression</tt> model trains on <tt>X</tt>, it finds those parameters using a method called Ordinary Least Squares, or OLS.\n",
    "\n",
    "As we'll see, finding the best parameters or algorithm for a machine learning model is not always as straight forward.\n",
    "\n",
    "## Part 2: Decision Tree Classification\n",
    "\n",
    "Now that we understand some of the concepts behind ML, we can move on to a more advanced use case. Let's define some of the core attributes:\n",
    "* **Task**: classifying features\n",
    "* **Model**: Decision Tree\n",
    "* **Loss function**: Gini Impurity\n",
    "\n",
    "Let's look at our data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = load_iris()\n",
    "print(iris_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a decision tree to classify plant features into one of three iris plant types:\n",
    "* Features:\n",
    " * Sepal Length\n",
    " * Sepal Width\n",
    " * Petal Length\n",
    " * Petal Width\n",
    "* Target\n",
    " * Iris Plant Type\n",
    " \n",
    "A decision tree is a tree-based model that uses splitting rules to predict a value for a given set of features. Like the parameters in a linear regression model, these splitting rules are learned by training on a labeled data set, and are chosen to optimize a loss function or metric (called criterion for scikit-learn's <tt>DecisionTreeClassifier</tt>).\n",
    "\n",
    "In this case, we use a metric called the \"gini impurity\". Put simply, it's a measurement of the likelihood of incorrect classification. The decision tree algorithm iteratively chooses values and features to split on in order to minimize this value.\n",
    "\n",
    "Let's train a decision tree classifier on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris_dataset['data'], iris_dataset['target'][:,np.newaxis]\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier()\n",
    "tree_classifier = tree_classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the built-in <tt>score</tt> method to calculate the accuracy of our trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy of trained decision tree: {100*tree_classifier.score(X, y)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's very good. It's actually **too good**... more on that soon.\n",
    "\n",
    "But first, now that we've trained the classifier, we can plot the decision tree and the splitting rules it learned for the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(tree_classifier, out_file=None, \n",
    "                     feature_names=iris_dataset['feature_names'],  \n",
    "                     class_names=iris_dataset['target_names'],  \n",
    "                     filled=True, rounded=True,  \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tree has, at maximum, 5 rules to get to a prediction. This is called the **max depth**.\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "Recall that our model achieved 100% accuracy on our training dataset. That sounds like a good thing, but it's actually not. When we train predictive models, we want them to be able to perform well on **unseen data**; that is, we want the rules the decision tree learns to **generalize** well to data it's never seen before. Our 100% accuracy means the model learned rules overly specific to the training set, but likely won't hold up to new data points. This phenomena is called **overfitting**.\n",
    "\n",
    "To visualize this, let's retrain the model using only two features so we can easily plot the decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualization code from https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html \n",
    "sample_X = iris_dataset.data[:, [0, 1]]\n",
    "sample_y = iris_dataset.target\n",
    "\n",
    "x_min, x_max = sample_X[:, 0].min() - 1, sample_X[:, 0].max() + 1\n",
    "y_min, y_max = sample_X[:, 1].min() - 1, sample_X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots()\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(sample_X, sample_y)\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axarr.contourf(xx, yy, Z, alpha=0.4)\n",
    "axarr.scatter(sample_X[:, 0], sample_X[:, 1], c=sample_y,\n",
    "                              s=20, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a decision tree classifier, our splitting rules divide the space into regions corresponding to each class. As you can see, the region is chopped up in such a way to ensure 100% accuracy, but it doesn't make much sense at all.\n",
    "\n",
    "Here's an example of a better decision tree classifier, this time with parameter <tt>max_depth=3</tt>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization code from https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html \n",
    "sample_X = iris_dataset.data[:, [0, 1]]\n",
    "sample_y = iris_dataset.target\n",
    "\n",
    "x_min, x_max = sample_X[:, 0].min() - 1, sample_X[:, 0].max() + 1\n",
    "y_min, y_max = sample_X[:, 1].min() - 1, sample_X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots()\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(sample_X, sample_y)\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axarr.contourf(xx, yy, Z, alpha=0.4)\n",
    "axarr.scatter(sample_X[:, 0], sample_X[:, 1], c=sample_y,\n",
    "                              s=20, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing this parameter, our boundaries seem more **generic**, rather than fitting to the random noise in the data.\n",
    "\n",
    "There are two approaches that together help overfitting:\n",
    "* Validation\n",
    "* Hyperparameter tuning\n",
    "\n",
    "Let's build a classifier on a more complicated data set using these techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_dataset = load_breast_cancer()\n",
    "print(breast_cancer_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = breast_cancer_dataset['data'], breast_cancer_dataset['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "We want our model to generalize well, thus we need to measure it's performance on data **it's never seen**. There are different ways to do this, but we'll use a simple one: we're going to hold out 20% of the training data and validate the model's performance on this hold out, which we call **validation data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier(random_state=1984)\n",
    "tree_classifier = tree_classifier.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare metrics between training data and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy on training data: {tree_classifier.score(train_X, train_y)}.\\\n",
    "  Accuracy on validation data: {tree_classifier.score(val_X, val_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, when left to overfit the model doesn't perform as well on data it's never seen before.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "How do we prevent our model from overfitting?\n",
    "\n",
    "When we train or fit our models, we use certain training algorithms that have their own set of rules or parameters. We call these **hyperparameters**, parameters that are not part of the model itself but change how the model is trained. The process of altering these hyperparameters to achieve the best validation performance for a model is called **hyperparameter tuning**, and it's at the core of machine learning.\n",
    "\n",
    "Let's look at the scikit-learn <tt>DecisionTreeClassifier</tt> [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) to understand some of the hyperparameters used in this algorithm.\n",
    "\n",
    "There are about 13 different hyperparameters. We can see by default, <tt>max_depth</tt> and <tt>min_samples_split</tt> are <tt>None</tt> and <tt>2</tt> respectively. These default settings almost guarantee that a decision tree will overfit, because there are no limitations on how deep to let the tree grow and it won't stop until every leaf (terminal node) of the tree has exactly one value in it.\n",
    "\n",
    "**Try changing the parameters below to find the best validation metric without overfitting. If you can, consider changing the below code to search over different parameter values. How might you determine whether a model is not overfitting?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=INSERT_CRITERION\n",
    "max_depth=INSERT_MAX_DEPTH\n",
    "min_samples_split=INSERT_MIN_SAMPLES_SPLIT\n",
    "min_samples_leaf=INSERT_MIN_SAMPLES_LEAF\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(criterion, max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         random_state=1984)\n",
    "tree_classifier = tree_classifier.fit(train_X, train_y)\n",
    "print(f\"Accuracy on training data: {tree_classifier.score(train_X, train_y)}.\\\n",
    "  Accuracy on validation data: {tree_classifier.score(val_X, val_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "We now know some of the core concepts that make up a machine learning problem:\n",
    "* The **task**, such as classification or regression\n",
    "* The **model**, such as a linear model or a decision tree\n",
    "* **Loss functions and metrics** (MSE, Gini Impurity, Accuracy)\n",
    "* **Validation** of models using a hold-out validation set\n",
    "* **Hyperparameter tuning** (max tree depth, etc.)\n",
    "\n",
    "Armed with this knowledge, it's time to move on to neural networks. \n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Artificial Neural Networks are machine learning models inspired by the human brain, and have been incredibly successful at a wide range of applications including computer vision and natural language processing. These models are **networks** of **neurons** that activate on inputs and work together to perform complex tasks.\n",
    "\n",
    "<figure><img src='https://user-images.githubusercontent.com/36491325/52151446-fc4d1180-2627-11e9-9ab7-7eab3af431fb.png'><figcaption>1. https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/</figcaption></figure>\n",
    "\n",
    "The circles above are these neurons. Features are fed into the network as indicated by the **input layer**, where each neuron is an individual feature. These input neurons then **feed-forward** through the network to the **first hidden layer**. If neurons in the hidden layer are activated, they feed-forward to the next hidden layer, and so on until the **output layer**. The output neurons are typically the **targets** the model is trying to predict.\n",
    "\n",
    "We call training neural networks **deep learning** because networks with many hidden layers are considered to be deep.\n",
    "\n",
    "### Neurons and Networks\n",
    "\n",
    "Let's take a closer look at neurons in an incredibly simple neural network, with one input neuron / feature, and one hidden neuron:\n",
    "\n",
    "![simplenn](https://user-images.githubusercontent.com/36491325/52152549-96fb1f80-262b-11e9-854d-2cd0eb651839.png)\n",
    "\n",
    "A neuron is a function of the neuron output before it. It's composed of a linear transformation and then a non-linear function, where the former transforms the input into a new feature space and the latter is used as an **activation function**. When neurons **activate**, their output moves forward in the network.\n",
    "\n",
    "Let's define the function for a neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(x, activation_function, params={\"m\":1.0, \"b\":0.0}):\n",
    "    linear_transformation = x * params['m'] + params['b']\n",
    "    activation = activation_function(linear_transformation)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks remarkably similar to our linear regression function, except now there is a non-linearity as well which we call the activation function. Let's define one common used for hidden layers, the rectified linear unit, or **ReLU**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.max([0, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU activates when the linear transformation is positive, and is zero otherwise.\n",
    "\n",
    "The output neurons typically have special activation functions depending on the task and the targets, we'll define a couple here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting them together, we can define an incredibly simple neural network object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(object):\n",
    "    \n",
    "    def __init__(self, hidden_neuron_activation_function, output_neuron_activation_function, params):\n",
    "        hidden_neuron_params = {\"m\":params[\"m_h\"], \"b\":params[\"b_h\"]}\n",
    "        output_neuron_params = {\"m\":params[\"m_o\"], \"b\":params[\"b_o\"]}\n",
    "        self.hidden_neuron = lambda x: neuron(x, hidden_neuron_activation_function, hidden_neuron_params)\n",
    "        self.output_neuron = lambda x: neuron(x, output_neuron_activation_function, output_neuron_params)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_neuron_activation = self.hidden_neuron(x)\n",
    "        output_neuron_activation = self.output_neuron(hidden_neuron_activation)\n",
    "        return output_neuron_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this simple network on the same data we trained a linear model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_dataset['data'][:,np.newaxis,2]\n",
    "y = diabetes_dataset['target'][:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, like our linear model we need to define the neural network parameters, or **weights**. I've randomly chosen the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdn_act_fn = relu\n",
    "out_act_fn = linear\n",
    "params = {\"m_h\":1.0, \"b_h\":3.0, \"m_o\":10.0, \"b_o\":0.0}\n",
    "\n",
    "simple_nn = SimpleNN(hdn_act_fn, out_act_fn, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call <tt>forward</tt> on our network to do a forward pass of new inputs through the network. Because we chose a linear output activation function, this network would be appropriate for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_nn.forward(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do a forward pass on all data points in <tt>X</tt>, we can calculate the MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MSE for this simple net is: {mse(np.array([simple_nn.forward(x) for x in X])[:,np.newaxis], y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did with our linear model,\n",
    "**try to pick the best combination of the 4 parameters for this network:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"m_h\":INSERT_MH, \"b_h\":INSERT_BH, \"m_o\":INSERT_MO, \"b_o\"INSERT_BO}\n",
    "\n",
    "simple_nn = SimpleNN(hdn_act_fn, out_act_fn, params)\n",
    "print(f\"MSE for this simple net is: {mse(np.array([simple_nn.forward(x) for x in X])[:,np.newaxis], y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a NN: Gradient Descent\n",
    "\n",
    "Now that we understand the basic structure of a very simple neural network and their parameters, you're probably asking the question **how do we find the best parameters**?\n",
    "\n",
    "Like our linear model, we find the best parameters by training the model on a data set and using a loss function. The \"best\" parameters are those that minimize the loss function.\n",
    "\n",
    "Let's return to that linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "m_range = np.concatenate([np.linspace(best_params['m'] - 10, best_params['m'], num=10),\n",
    "                          np.linspace(best_params['m'] + 1, best_params['m'] + 11, num=10)])\n",
    "\n",
    "b_range = np.concatenate([np.linspace(best_params['b'] - 1, best_params['b'], num=10),\n",
    "                          np.linspace(best_params['b'], best_params['b'] + 1, num=10)])\n",
    "\n",
    "m_grid, b_grid = np.meshgrid(m_range, b_range)\n",
    "zs = []\n",
    "for m,b in zip(np.ravel(m_grid), np.ravel(b_grid)):\n",
    "    linear_model = create_linear_model({\"m\":m, \"b\":b})\n",
    "    tmp_preds = np.array([linear_model(x) for x in X])\n",
    "    zs.append(mse(tmp_preds, y))\n",
    "zs = np.array(zs)\n",
    "Z = zs.reshape(m_grid.shape)\n",
    "\n",
    "ax.plot_wireframe(m_grid, b_grid, Z)\n",
    "\n",
    "ax.set_xlabel('m Values')\n",
    "ax.set_ylabel('b Values')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model only has two parameters, or weights. By plotting the loss value for different combinations of the two, we can visualize the **loss surface** and can clearly see the minima. But how do we calculate these values?\n",
    "\n",
    "In linear regression, there is a way to explicity solve for these values called **Ordinary Least Squares** that under certain assumptions is mathematically guaranteed to give you the best parameters. When we use scikit-learn's <tt>LinearRegression</tt> class and call <tt>fit</tt> on our data, it's using OLS to find the best parameters which we can plot on the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "best_params = {\"m\":lr_model.coef_[0,0], \"b\":lr_model.intercept_[0]}\n",
    "best_line_function = create_linear_model(best_params)\n",
    "best_preds = np.array([best_line_function(x) for x in X])\n",
    "best_loss = mse(best_preds, y)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(m_grid, b_grid, Z)\n",
    "ax.scatter(best_params['m'], best_params['b'], best_loss, s=100, marker=(5,1), c='r')\n",
    "ax.text(best_params['m'], best_params['b'], best_loss, \"Best Parameters\", zorder=1)\n",
    "\n",
    "ax.set_xlabel('m Values')\n",
    "ax.set_ylabel('b Values')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is no parallel to OLS for neural networks, and instead optimal parameters are found through an optimization algorithm called **gradient descent**. Put simply, gradient descent works by starting with some random combination of parameters, and then moving each parameter down the slope of the loss surface to get closer to a minima.\n",
    "\n",
    "#### Deep Dive on Gradient Descent\n",
    "First, let's define a gradient. The gradient of a function is the multi-dimensional version of the **derivative**, which you may remember from calculus tells you the **rate of change** in a function for a specific point $x$. This can also be visualized as the slope of the tangent line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_title(\"f(x)=x^2\")\n",
    "ax.plot(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100)**2)\n",
    "ax.scatter(0.25, 0.25**2, c=\"r\")\n",
    "ax.plot(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100)*0.5 - 0.0625)\n",
    "ax.text(0.25, 0.25**2, \"Derivative f'(0.25) is 0.5,\\n slope of tangent line\")\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "ax.set_ylabel(\"f(x)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_title(\"f(x)=x^2\")\n",
    "ax.plot(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100)**2)\n",
    "ax.scatter(-0.5, 0.5**2, c=\"r\")\n",
    "ax.plot(np.linspace(-1, 1, 100), np.linspace(-1, 1, 100)*-1 - 0.25)\n",
    "ax.text(-0.5, 0.5**2, \"Derivative f'(-0.5) is -1,\\n slope of tangent line\")\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative tells you the tangential slope of the function at a given point. In the case of $f(x)=x^2$, you can see that whatever the derivative is, the minima is in **the opposite direction**. For example, the derivative $f'(0.25)$ is **positive**, and so the function is increasing and we want to move in the **negative** direction to find the minima. The same logic applies to $f'(-0.5)$\n",
    "\n",
    "This logic is the crux of gradient descent, only now the function we want to minimize is the loss function $L$, and we take the derivatives of $L$ with respect to the model parameters. \n",
    "\n",
    "#### Gradient Descent on Linear Model\n",
    "\n",
    "For a simple linear model, the algorithm works like this:\n",
    "\n",
    "1. Choose random values for parameters m and b\n",
    "2. Calculate f(x)=mx+b for all x in X\n",
    "3. Calculate the mean-squared-error between model predictions and the true labels\n",
    "4. Take the derivatives (called the gradient) of the loss function with respect to m and b\n",
    "5. Update m by moving it in the opposite direction of the gradient\n",
    "6. Update b by moving in the opposite direction of the gradient\n",
    "7. Repeat steps 2-7 until the change in m and b is sufficiently small\n",
    "\n",
    "When the parameters stop updating, the model has said to have **converged**. \n",
    "\n",
    "Mathematically, we can write out the update steps for $m$ and $b$:\n",
    "\n",
    "$m_{n+1} = m_{n} - \\gamma \\frac{\\partial}{\\partial m} L$\n",
    "\n",
    "$b_{n+1} = b_{n} - \\gamma \\frac{\\partial}{\\partial b} L $\n",
    "\n",
    "where $\\frac{\\partial}{\\partial x}$ is the **partial derivative** with respect to $x$. Both of these equations show how m and b are **updated** at each step n by moving in the opposite direction of the gradient. \n",
    "\n",
    "For an arbitrary number of parameters $\\theta$, we can write this update equation as\n",
    "\n",
    "$\\theta_{n+1}=\\theta_{n} - \\gamma \\nabla L$ where $\\nabla$ is the gradient operator.\n",
    "\n",
    "$\\gamma$ in this equation is a very important hyperparameter that we call the step size or more commonly the **learning rate**. This determines how big of a step you take in each update. The gradient is only the tangential rate of change, the further you move away from where it was calculated the more room for error. A larger learning rate can cause faster convergence, but it can also be inaccurate. A smaller learning rate will be more accurate but can be slow.\n",
    "\n",
    "To get a better understanding, let's perform this process manually. Looking at our data, we can make an educated guess for m and b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_m = 943\n",
    "start_b = 151.50\n",
    "start_params = {\"m\":start_m, \"b\":start_b}\n",
    "\n",
    "\n",
    "start_line_function = create_linear_model(start_params)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, y)\n",
    "# ax.set_aspect('equal')\n",
    "ax.grid(True, which='both')\n",
    "\n",
    "ax.axhline(y=0, color='k')\n",
    "ax.axvline(x=0, color='k')\n",
    "start = np.min(X)\n",
    "stop = np.max(X)\n",
    "start_preds = np.array([start_line_function(x) for x in X])\n",
    "ax.set_title(f\"Linear model y = {start_params['m']}*x + {start_params['b']}\")\n",
    "ax.set_xlabel(\"BMI\")\n",
    "ax.set_ylabel(\"Diabetes Progression\")\n",
    "ax.plot(X, start_preds, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with this guess, let's calculate the loss for a linear model with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_preds = np.array([start_line_function(x) for x in X])\n",
    "start_loss = mse(start_preds, y)\n",
    "print(f\"Initial loss of {start_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where this is on the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_wireframe(m_grid, b_grid, Z)\n",
    "ax.scatter(start_params['m'], start_params['b'], start_loss, s=100, marker=(5,1), c='r')\n",
    "ax.text(start_params['m'], start_params['b'], start_loss, \"Start Parameters\", zorder=1)\n",
    "\n",
    "ax.set_xlabel('m Values')\n",
    "ax.set_ylabel('b Values')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the axes, it's clear that both $m$ and $b$ need to increase to move closer to the minima, and this will be reflected in gradient descent by a negative gradient\n",
    "\n",
    "In order to figure out which direction to move $m$ and $b$, gradient descent finds the gradient of the loss function. We can get this by calculating the partial derivatives of the mean squared error for both $m$ and $b$. These are\n",
    "\n",
    "$\\frac{\\partial}{\\partial m}L=\\frac{2}{N}\\sum_i^N x(mx+b-y)$\n",
    "\n",
    "$\\frac{\\partial}{\\partial b}L=\\frac{2}{N}\\sum_i^N (mx+b-y)$\n",
    "\n",
    "which we can define below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_mse_m(X, y, params):\n",
    "    return 2*np.mean(X*(params['m']*X + params['b'] - y))\n",
    "\n",
    "def pd_mse_b(X, y, params):\n",
    "    return 2*np.mean(params['m']*X + params['b'] - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_m = pd_mse_m(X, y, start_params)\n",
    "pd_b = pd_mse_b(X, y, start_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Partial derivatives of the loss function w.r.t\\nm: {pd_m},\\nb: {pd_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, they are both negative. We can now update our parameters; we will use a learning rate of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1.0\n",
    "\n",
    "new_m = start_m - lr*pd_m\n",
    "new_b = start_b - lr*pd_b\n",
    "new_params = {\"m\":new_m, \"b\":new_b}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the new loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_line_function = create_linear_model(new_params)\n",
    "new_preds = np.array([new_line_function(x) for x in X])\n",
    "new_loss = mse(new_preds, y)\n",
    "print(f\"Initial loss of {start_loss} improved to {new_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the loss has decreased as expected. Let's create a function for this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, start_params, lr, epochs=1000):\n",
    "    old_params = start_params\n",
    "    old_linear_model = create_linear_model(old_params)\n",
    "    old_preds = np.array([old_linear_model(x) for x in X])\n",
    "    old_loss = mse(old_preds, y)\n",
    "    i = 0\n",
    "    ms = [old_params['m']]\n",
    "    bs = [old_params['b']]\n",
    "    losses = [old_loss]\n",
    "    for i in range(epochs):\n",
    "        pd_m = pd_mse_m(X, y, old_params)\n",
    "        pd_b = pd_mse_b(X, y, old_params)\n",
    "        new_m = old_params['m'] - lr*pd_m\n",
    "        new_b = old_params['b'] - lr*pd_b\n",
    "        new_params = {\"m\":new_m, \"b\":new_b}\n",
    "        new_linear_model = create_linear_model(new_params)\n",
    "        new_preds = np.array([new_linear_model(x) for x in X])\n",
    "        new_loss = mse(new_preds, y)\n",
    "        ms.append(new_params['m'])\n",
    "        bs.append(new_params['b'])\n",
    "        losses.append(new_loss)\n",
    "        old_params = new_params\n",
    "        old_preds = new_preds\n",
    "    return new_params, ms, bs, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run the algorithm for 1000 iterations/updates, which are called **epochs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "gd_params, ms, bs, losses = gradient_descent(X, y, start_params, lr=0.01, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these results compare with the OLS estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After {epochs} iterations of gradient descent:\\\n",
    "\\nStart params: {start_params}\\nG.D. params:{gd_params}\\nOLS best params:{best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loss over iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"MSE per iteration of Gradient Descent\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after 100 epochs or so, the loss curve starts to flatten out. This pattern of diminishing returns is a strong indicator that gradient descent has converged. While a larger learning rate can cause quicker convergence, if it's too big the algorithm will overshoot and the loss will get even bigger.\n",
    "\n",
    "Let's visualize the parameter updates on the loss surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization code from https://jed-ai.github.io/py1_gd_animation/\n",
    "fig1 = plt.figure()\n",
    "ax1 = Axes3D(fig1)\n",
    "surf = ax1.plot_wireframe(m_grid, b_grid, Z, rstride=1,\n",
    "                        cstride=1)\n",
    "\n",
    "# Plot target (the minimum of the function)\n",
    "ax1.scatter(best_params['m'], best_params['b'], best_loss, s=100, marker=(5,1), c='r')\n",
    "\n",
    "ax1.set_xlabel('m Values')\n",
    "ax1.set_ylabel('b Values')\n",
    "ax1.set_zlabel('MSE')\n",
    "\n",
    "line, = ax1.plot([], [], [], 'r-', label = 'Gradient descent', lw = 1.5)\n",
    "point, = ax1.plot([], [], [], 'bo')\n",
    "display_value = ax1.text(2., 2., 27.5, '', transform=ax1.transAxes)\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    line.set_3d_properties([])\n",
    "    point.set_data([], [])\n",
    "    point.set_3d_properties([])\n",
    "    display_value.set_text('')\n",
    "\n",
    "    return line, point, display_value\n",
    "\n",
    "def animate(i):\n",
    "    # Animate line\n",
    "    line.set_data(ms[:i], bs[:i])\n",
    "    line.set_3d_properties(losses[:i])\n",
    "    \n",
    "    # Animate points\n",
    "    point.set_data(ms[i], bs[i])\n",
    "    point.set_3d_properties(losses[i])\n",
    "\n",
    "    # Animate display value\n",
    "    display_value.set_text('Min = ' + str(losses[i]))\n",
    "\n",
    "    return line, point, display_value\n",
    "\n",
    "ax1.legend(loc = 1)\n",
    "\n",
    "anim = animation.FuncAnimation(fig1, animate, init_func=init,\n",
    "                               frames=len(ms), interval=120, \n",
    "                               repeat_delay=60, blit=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that at first the algorithm moves quickly down the slope, but once it reaches the bottom it starts to move slowly. This is because the loss changes more slowly in along the $m$ axis. Different implementations of the gradient descent algorithm help mitigate this problem.\n",
    "\n",
    "**Try running gradient descent with different learning rates between 0.0001 and 1.0. Plot the parameter trajectory to see how smaller and larger learning rates affect convergence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = INSERT_LR\n",
    "gd_params, ms, bs, losses = gradient_descent(X, y, start_params, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After {epochs} iterations of gradient descent:\\\n",
    "\\nStart params: {start_params}\\nG.D. params:{gd_params}\\nOLS best params:{best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(f\"MSE per iteration of Gradient Descent\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization code from https://jed-ai.github.io/py1_gd_animation/\n",
    "fig1 = plt.figure()\n",
    "ax1 = Axes3D(fig1)\n",
    "surf = ax1.plot_wireframe(m_grid, b_grid, Z, rstride=1,\n",
    "                        cstride=1)\n",
    "\n",
    "# Plot target (the minimum of the function)\n",
    "ax1.scatter(best_params['m'], best_params['b'], best_loss, s=100, marker=(5,1), c='r')\n",
    "\n",
    "ax1.set_xlabel('m Values')\n",
    "ax1.set_ylabel('b Values')\n",
    "ax1.set_zlabel('MSE')\n",
    "\n",
    "line, = ax1.plot([], [], [], 'r-', label = 'Gradient descent', lw = 1.5)\n",
    "point, = ax1.plot([], [], [], 'bo')\n",
    "display_value = ax1.text(2., 2., 27.5, '', transform=ax1.transAxes)\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    line.set_3d_properties([])\n",
    "    point.set_data([], [])\n",
    "    point.set_3d_properties([])\n",
    "    display_value.set_text('')\n",
    "\n",
    "    return line, point, display_value\n",
    "\n",
    "def animate(i):\n",
    "    # Animate line\n",
    "    line.set_data(ms[:i], bs[:i])\n",
    "    line.set_3d_properties(losses[:i])\n",
    "    \n",
    "    # Animate points\n",
    "    point.set_data(ms[i], bs[i])\n",
    "    point.set_3d_properties(losses[i])\n",
    "\n",
    "    # Animate display value\n",
    "    display_value.set_text('Min = ' + str(losses[i]))\n",
    "\n",
    "    return line, point, display_value\n",
    "\n",
    "ax1.legend(loc = 1)\n",
    "\n",
    "anim = animation.FuncAnimation(fig1, animate, init_func=init,\n",
    "                               frames=len(ms), interval=120, \n",
    "                               repeat_delay=60, blit=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent on our Simple Neural Network\n",
    "\n",
    "Now that we understand the fundamental concepts behind gradient descent, we can use this algorithm to learn the parameters for our simple neural network. This time we have four parameters $m_h$, $b_h$, $m_o$, and $b_o$, two for each neuron, which means we need four partial derivatives:\n",
    "\n",
    "$\\frac{\\partial}{\\partial m_h} L= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 \\text{ if } m_h x + b_h \\leq 0 \\\\\n",
    "      \\frac{2m_o}{N}\\sum_i^N x (m_o(m_h x + b_h) + b_o - y)\\\\\n",
    "\\end{array} \n",
    "\\right.$\n",
    "\n",
    "$\\frac{\\partial}{\\partial b_h} L= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 \\text{ if } m_h x + b_h \\leq 0 \\\\\n",
    "      \\frac{2m_o}{N}\\sum_i^N (m_o(m_h x + b_h) + b_o - y)\\\\\n",
    "\\end{array} \n",
    "\\right.$\n",
    "\n",
    "$\\frac{\\partial}{\\partial m_o} L= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      0 \\text{ if } m_h x + b_h \\leq 0 \\\\\n",
    "      \\frac{2}{N}\\sum_i^N ( m_h x +b_h) (m_o(m_h x +b_h) +b_o - y)\\\\\n",
    "\\end{array} \n",
    "\\right.$\n",
    "\n",
    "$\\frac{\\partial}{\\partial b_o} L= \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{2}{N}\\sum_i^N (b_o - y) \\text{ if } m_h x + b_h \\leq 0 \\\\\n",
    "      \\frac{2}{N}\\sum_i^N (m_o (m_h x + b_h) + b_o - y)\\\\\n",
    "\\end{array} \n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_mse_mh(X, y, params):\n",
    "    h_linear = X*params['m_h'] + params['b_h']\n",
    "    zero_mask = (h_linear >= 0).astype(int)\n",
    "    return 2*params['m_o']*np.mean((X*(params['m_o'] * h_linear + params['b_o'] - y))*zero_mask)\n",
    "\n",
    "def pd_mse_bh(X, y, params):\n",
    "    h_linear = X*params['m_h'] + params['b_h']\n",
    "    zero_mask = (h_linear >= 0).astype(int)\n",
    "    return 2*params['m_o']*np.mean((params['m_o'] * h_linear + params['b_o'] - y)*zero_mask)\n",
    "\n",
    "def pd_mse_mo(X, y, params):\n",
    "    h_linear = X*params['m_h'] + params['b_h']\n",
    "    zero_mask = (h_linear >= 0).astype(int)\n",
    "    return 2*np.mean((h_linear*(params['m_o'] * h_linear + params['b_o'] - y))*zero_mask)\n",
    "\n",
    "    \n",
    "def pd_mse_bo(X, y, params):\n",
    "    h_linear = X*params['m_h'] + params['b_h']\n",
    "    zero_indices = (h_linear < 0)\n",
    "    out = (params['m_o'] * h_linear + params['b_o'] - y)\n",
    "    out[zero_indices] = 2*(params['b_o'] - y[zero_indices])\n",
    "    return 2*np.mean(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our algorithm from before to account for all four parameter updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_net(X, y, start_params, lr, epochs=1000):\n",
    "    old_params = start_params\n",
    "    old_simple_net = SimpleNN(relu, linear, old_params)\n",
    "    old_preds = np.array([old_simple_net.forward(x) for x in X])\n",
    "    old_loss = mse(old_preds, y)\n",
    "    i = 0\n",
    "    losses = [old_loss]\n",
    "    for i in range(epochs):\n",
    "        pd_mh = pd_mse_mh(X, y, old_params)\n",
    "        pd_bh = pd_mse_bh(X, y, old_params)\n",
    "        pd_mo = pd_mse_mo(X, y, old_params)\n",
    "        pd_bo = pd_mse_bo(X, y, old_params)\n",
    "        new_mh = old_params['m_h'] - lr*pd_mh\n",
    "        new_bh = old_params['b_h'] - lr*pd_bh\n",
    "        new_mo = old_params['m_o'] - lr*pd_mo\n",
    "        new_bo = old_params['b_o'] - lr*pd_bo\n",
    "        new_params = {\"m_h\":new_mh, \"b_h\":new_bh, \"m_o\":new_mo, \"b_o\":new_bo}\n",
    "        new_simple_net = SimpleNN(relu, linear, new_params)\n",
    "        new_preds = np.array([new_simple_net.forward(x) for x in X])\n",
    "        new_loss = mse(new_preds, y)\n",
    "        losses.append(new_loss)\n",
    "        old_params = new_params\n",
    "        old_preds = new_preds\n",
    "    return new_params, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "start_net_params = {'m_h': 1.0, 'b_h': 3.0, 'm_o': 10.0, 'b_o': 5.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, losses = gradient_descent_net(X, y, start_net_params, 0.001, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient descent for {epochs} epochs.\\nStarting MSE: {losses[0]}\\nFinal MSE: {losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"MSE per iteration of Gradient Descent\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can see that the model is indeed learning! But it's actually performing **worse** than the linear model. Why?\n",
    "\n",
    "Unlike the linear model, the loss function for this neural network is **bumpy**. In other words, it has lots of peaks and valleys, and the gradient descent algorithm can get **stuck** in a **local minima**. Our selection of initial parameters as well as the learning rate have a big impact on how well gradient descent works, which is one reason why hyperparameter tuning is so important in deep learning.\n",
    "\n",
    "**Try choosing your own initial parameters and learning rate to get the best results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = INSERT_LR\n",
    "epochs = INSERT_EPOCHS\n",
    "start_net_params = {'m_h': INSERT_MH, 'b_h': INSERT_BH, 'm_o': INSERT_MO, 'b_o': INSERT_BO}\n",
    "params, losses = gradient_descent_net(X, y, start_net_params, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gradient descent for {epochs} epochs.\\nStarting MSE: {losses[0]}\\nFinal MSE: {losses[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"MSE per iteration of Gradient Descent\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(range(len(losses)), losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, picking optimal starting parameters and learning rate for training neural networks is difficult. When your network has millions of parameters, as most do, it becomes downright impossible. Fortunately, much work has been done to help make this process easier, as well see in the next section.\n",
    "\n",
    "### Deep Learning Frameworks: TensorFlow + Keras\n",
    "\n",
    "Our simple neural network had exactly one feature, and one hidden layer with one neuron. Typically, neural networks have millions of parameters, and this presents many challenges. Deep Learning frameworks such as Tensorflow, PyTorch, and MXNet have sprung up to help solve these challenges. Deep learning frameworks:\n",
    "* Provide convenient abstractions for neural networks\n",
    "* Automate partial differentiation for parameter update\n",
    "* Allows for accelerated compute, leveraging CuDNN and CUDA to train neural networks on GPU's\n",
    "\n",
    "That last two points are critical. Automatically differentiating millions of parameters is a monumental task, and oftentimes it's not feasible to train a neural network on a CPU. The high-throughput GPUs allow for make them ideal for training neural networks, and deep learning frameworks are designed to support this acceleration. However, there are many different ways to support this, which is why there are many different frameworks available.\n",
    "\n",
    "We're going to use Tensorflow in this example, but rather than use native TF we're going to use the Keras frontend. The Keras API provides convenient abstraction at the layer level, and is very user-friendly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look at the diabetes dataset again, but this time we're going to use all ten features as inputs. Then we'll do a split as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_dataset['data']\n",
    "y = diabetes_dataset['target'][:,np.newaxis]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to create a neural network using Keras. This time however, we won't need to pick initial parameters. Keras, and other deep learning frameworks, use special heuristics to initialize parameters in a \"good\" way. We're also going to use a variant of gradient descent called **Adam**, which attempts to mitigate some of the issues we encountered earlier. \n",
    "\n",
    "We're going to introduce one more concept before we start training our model. Earlier we talked about the problem of **overfitting**; neural networks are particularly prone to this. Techniques for preventing overfitting are called **regularization**, and a popular regularization technique in deep learning is called **dropout**. Dropout works by setting individual neurons to zero with some probability. This forces the network to learn more general parameters by preventing what's called complex co-adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(input_configuration, input_shape, drop_prob=0.5, lr=0.001, beta_1=0.9, beta_2=0.999, decay=0.0):\n",
    "\n",
    "    model = Sequential()\n",
    "    for i, config in enumerate(input_configuration):\n",
    "        if i == 0:\n",
    "            model.add(Dense(config[0], activation=config[1], input_dim=input_shape))\n",
    "        else:\n",
    "            model.add(Dense(config[0], activation=config[1]))\n",
    "        model.add(Dropout(drop_prob))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    adam = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, decay=decay)\n",
    "    model.compile(loss='mse', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to have the following hyperparameters to tune:\n",
    "* <tt>input_configuration</tt>: A list of the form <tt>[[num_hidden_units, activation]]</tt> with as many for the number of layers you want.\n",
    "* <tt>drop_prob</tt>: The probability of dropout applied at each layer\n",
    "* <tt>lr</tt>, <tt>beta_1</tt>, <tt>beta_2</tt>, <tt>decay</tt>: Parameters for the Adam optimizer\n",
    "\n",
    "There are additional parameters you can change as well, which you can learn more about in the [documentation](https://keras.io/layers/core/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network([[200, \"relu\"], [100, \"relu\"]], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to train the model. Note that with neural networks, we typically update parameters on \"batches\" of data at a time, rather than the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, validation_data=(val_X, val_y),\n",
    "          epochs=200,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot our loss over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"MSE per Epoch\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.plot(range(len(history.history['loss'])), history.history['loss'], label=\"Training Loss\")\n",
    "ax.plot(range(len(history.history['loss'])), history.history['val_loss'], label=\"Validation Loss\")\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try your own combinations of hyperparameters to achieve the best validation loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_network(INSERT_NETWORK_CONFIG)\n",
    "history = model.fit(train_X, train_y, validation_data=(val_X, val_y),\n",
    "          epochs=200,\n",
    "          batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"MSE per Epoch\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.plot(range(len(history.history['loss'])), history.history['loss'], label=\"Training Loss\")\n",
    "ax.plot(range(len(history.history['loss'])), history.history['val_loss'], label=\"Validation Loss\")\n",
    "fig.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
